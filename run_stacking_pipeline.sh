#!/bin/bash
# ë©€í‹°ëª¨ë¸ ìŠ¤íƒœí‚¹ íŒŒì´í”„ë¼ì¸: CatBoost + LightGBM + XGBoost 5-Fold OOF â†’ ë©”íƒ€ ë¡œì§€ìŠ¤í‹± íšŒê·€
# ì‚¬ìš©: bash run_stacking_pipeline.sh [TOP_N(optional)]
# TOP_N ì£¼ë©´ feature_selection ê²°ê³¼ ìƒìœ„ í”¼ì²˜ë§Œ ì‚¬ìš© (ì—†ìœ¼ë©´ ì „ì²´)

set -e
TOP_N=${1:-0}

echo "ğŸš€ ë©€í‹°ëª¨ë¸ ìŠ¤íƒœí‚¹ íŒŒì´í”„ë¼ì¸ ì‹œì‘ (TOP_N=${TOP_N})"

python <<PY
import os, pickle, numpy as np, pandas as pd, time
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score

import sys
sys.path.append('utils'); sys.path.append('features')
from common_utils import load_data, evaluate_model
from feature_engineering import build_a_features, build_b_features, merge_features_with_labels
from preprocessing import FeaturePreprocessor

from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

print('[1] ë°ì´í„° ë¡œë“œ & í”¼ì²˜ ìƒì„±')
train, test, train_a, train_b, test_a, test_b = load_data()
fa = build_a_features(train_a)
fb = build_b_features(train_b)
X_raw, y = merge_features_with_labels(train, fa, fb)

print('[2] ì „ì²˜ë¦¬ê¸° ì í•©')
pre = FeaturePreprocessor().fit(X_raw)
X_full = pre.transform(X_raw)
features_all = X_full.columns.tolist()

if ${TOP_N} > 0 and os.path.exists('output/feature_importance_mean.csv'):
    imp_df = pd.read_csv('output/feature_importance_mean.csv')
    selected = imp_df.head(int(${TOP_N}))['feature'].tolist()
    # ì„ íƒëœ ì»¬ëŸ¼ì´ ì „ì²˜ë¦¬ ê²°ê³¼ì— ì—†ëŠ” ê²½ìš° í•„í„°ë§
    selected = [c for c in selected if c in features_all]
    X_full = X_full[selected]
    print(f'TOP_N={${TOP_N}} ì ìš© í›„ Shape: {X_full.shape}')
else:
    print('ì „ì²´ í”¼ì²˜ ì‚¬ìš©')

K=5
skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)

print('[3] Base ëª¨ë¸ OOF ìƒì„±')
cat_oof = np.zeros(len(y)); lgb_oof = np.zeros(len(y)); xgb_oof = np.zeros(len(y))
cat_models=[]; lgb_models=[]; xgb_models=[]

for fold,(tr_idx,val_idx) in enumerate(skf.split(X_full,y),1):
    print(f'\nâ–¶ Fold {fold}/{K}')
    X_tr, X_val = X_full.iloc[tr_idx], X_full.iloc[val_idx]
    y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]

    cat = CatBoostClassifier(iterations=400, depth=6, learning_rate=0.05, auto_class_weights='Balanced', random_seed=fold+10, verbose=100)
    cat.fit(X_tr,y_tr, eval_set=(X_val,y_val), early_stopping_rounds=40)
    cat_p = cat.predict_proba(X_val)[:,1]
    cat_oof[val_idx] = cat_p
    cat_models.append(cat)

    lgb = LGBMClassifier(n_estimators=400, learning_rate=0.05, max_depth=-1, subsample=0.8, colsample_bytree=0.8, random_state=fold+20, verbose=-1)
    lgb.fit(X_tr,y_tr, eval_set=[(X_val,y_val)], eval_metric='auc')
    lgb_p = lgb.predict_proba(X_val)[:,1]
    lgb_oof[val_idx] = lgb_p
    lgb_models.append(lgb)

    xgb = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=fold+30, eval_metric='auc', use_label_encoder=False, verbosity=0)
    xgb.fit(X_tr,y_tr, eval_set=[(X_val,y_val)])
    xgb_p = xgb.predict_proba(X_val)[:,1]
    xgb_oof[val_idx] = xgb_p
    xgb_models.append(xgb)

print('\nBase ëª¨ë¸ OOF AUCë“¤:')
print(' CatBoost AUC:', roc_auc_score(y, cat_oof))
print(' LightGBM AUC:', roc_auc_score(y, lgb_oof))
print(' XGBoost  AUC:', roc_auc_score(y, xgb_oof))

print('[4] ë©”íƒ€ ë°ì´í„° êµ¬ì„±')
meta_X = pd.DataFrame({'cat':cat_oof,'lgb':lgb_oof,'xgb':xgb_oof})
meta_y = y.copy()

print('[5] ë©”íƒ€ ë¡œì§€ìŠ¤í‹± íšŒê·€ í•™ìŠµ (5-Fold ë‚´ë¶€ OOF ì‚¬ìš©)')
# ê°„ë‹¨íˆ ì „ì²´ OOFë¡œ í•™ìŠµ (Nested CV ìƒëµ)
stack_model = LogisticRegression(max_iter=1000)
stack_model.fit(meta_X, meta_y)
meta_proba = stack_model.predict_proba(meta_X)[:,1]
auc_stack = roc_auc_score(meta_y, meta_proba)
print(f'Stacking OOF AUC: {auc_stack:.4f}')

# ì„ê³„ê°’ ë‹¤ë³€í™” ë™ì¼ ë¡œì§ ì ìš©
thresholds = np.arange(0.01,0.99,0.01)
recs=[]
for t in thresholds:
    yb=(meta_proba>=t).astype(int)
    prec=precision_score(meta_y,yb,zero_division=0)
    rec=recall_score(meta_y,yb,zero_division=0)
    f1=f1_score(meta_y,yb,zero_division=0)
    tp=((meta_y==1)&(yb==1)).sum(); fn=((meta_y==1)&(yb==0)).sum(); fp=((meta_y==0)&(yb==1)).sum(); tn=((meta_y==0)&(yb==0)).sum()
    tpr=tp/(tp+fn) if (tp+fn)>0 else 0; fpr=fp/(fp+tn) if (fp+tn)>0 else 0
    youden=tpr-fpr
    recs.append({'threshold':t,'precision':prec,'recall':rec,'f1':f1,'youden':youden})
thr_df=pd.DataFrame(recs)
thr_df.to_csv('output/thresholds_stacking_oof.csv',index=False)

best_f1_thr=float(thr_df.loc[thr_df['f1'].idxmax()].threshold)
candidate=thr_df[thr_df['precision']>=0.08]
if candidate.empty:
    best_rp_thr=best_f1_thr
    best_rp_row=thr_df.loc[thr_df['f1'].idxmax()]
else:
    best_rp_row=candidate.sort_values('recall',ascending=False).iloc[0]
    best_rp_thr=float(best_rp_row.threshold)

best_youden_thr=float(thr_df.loc[thr_df['youden'].idxmax()].threshold)
final_thr=best_rp_thr if not candidate.empty else best_f1_thr if thr_df.loc[thr_df['f1'].idxmax()].f1 >= thr_df.loc[thr_df['youden'].idxmax()].f1 else best_youden_thr
print(f'Staking ìµœì¢… ì„ê³„ê°’: {final_thr:.3f}')

# ì €ì¥
with open('output/models/stack_preprocessor.pkl','wb') as f:
    pickle.dump(pre,f)
with open('output/models/stack_cat_models.pkl','wb') as f:
    pickle.dump(cat_models,f)
with open('output/models/stack_lgb_models.pkl','wb') as f:
    pickle.dump(lgb_models,f)
with open('output/models/stack_xgb_models.pkl','wb') as f:
    pickle.dump(xgb_models,f)
with open('output/models/stack_meta_model.pkl','wb') as f:
    pickle.dump(stack_model,f)
with open('output/models/stack_final_threshold.pkl','wb') as f:
    pickle.dump(final_thr,f)

pd.DataFrame([{'auc_cat':roc_auc_score(y,cat_oof),'auc_lgb':roc_auc_score(y,lgb_oof),'auc_xgb':roc_auc_score(y,xgb_oof),'auc_stack':auc_stack,'final_thr':final_thr,'top_n':${TOP_N}}]).to_csv('output/stacking_summary.csv',index=False)
print('\nâœ… ìŠ¤íƒœí‚¹ í•™ìŠµ ì™„ë£Œ: stacking_summary.csv ì €ì¥')
PY

echo "ğŸ‰ ìŠ¤íƒœí‚¹ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (stacking_summary.csv, ëª¨ë¸/ì„ê³„ê°’ ì €ì¥)"
